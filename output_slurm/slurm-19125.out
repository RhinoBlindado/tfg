------------ Options -------------
amsgrad: False
arch: mconvnet
batch_size: 1
beta1: 0.9
checkpoints_dir: ./checkpoints
continue_train: False
csv: False
dataroot: ./data/datasets/ResTest-30K
dataset_mode: classification
dropout: [0]
epoch_count: 1
export_folder: 
fc_n: [100]
flip_edges: 0.0
gpu_ids: [0]
init_gain: 0.02
init_type: normal
is_train: True
lr: 0.0002
lr_decay_iters: 50
lr_policy: lambda
max_dataset_size: inf
name: MeshCN-ResTest-30K
ncf: [64, 128, 256, 256]
ninput_edges: 30000
niter: 3
niter_decay: 0
no_vis: False
norm: batch
num_aug: 1
num_groups: 1
num_threads: 2
optimizer: adam
phase: train
plus: False
pool_res: [24000, 18000, 12000, 7200]
print_freq: 9999
resblocks: 0
run_test_freq: 9999
save_epoch_freq: 1
save_latest_freq: 250
scale_verts: False
seed: 16
serial_batches: False
slide_verts: 0.0
validation: False
verbose_plot: False
verbose_train: False
which_epoch: latest
-------------- End ----------------
MeshCNN layers: Using sparse pooling
loaded mean / std from cache
#training meshes = 1
Using distributed classifier model with AdamW optimizer, GPU 0
---------- Network initialized -------------
[Network] Total number of parameters : 0.561 M
-----------------------------------------------
/mnt/homeGPU/vlugli/condaEnvs/meshcnnplus/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order, subok=True)
saving the latest model (epoch 1, total_steps 1)
saving the model at the end of epoch 1, iters 1
End of epoch 1 / 3 	 Time Taken: 26 sec
learning rate = 0.0002000
saving the latest model (epoch 2, total_steps 2)
saving the model at the end of epoch 2, iters 2
End of epoch 2 / 3 	 Time Taken: 21 sec
learning rate = 0.0000000
saving the latest model (epoch 3, total_steps 3)
saving the model at the end of epoch 3, iters 3
End of epoch 3 / 3 	 Time Taken: 22 sec
learning rate = -0.0002000
------------ Options -------------
amsgrad: False
arch: mconvnet
batch_size: 1
beta1: 0.9
checkpoints_dir: ./checkpoints
continue_train: False
csv: False
dataroot: ./data/datasets/ResTest-50K
dataset_mode: classification
dropout: [0]
epoch_count: 1
export_folder: 
fc_n: [100]
flip_edges: 0.0
gpu_ids: [0]
init_gain: 0.02
init_type: normal
is_train: True
lr: 0.0002
lr_decay_iters: 50
lr_policy: lambda
max_dataset_size: inf
name: MeshCNNPlus-ResTest-50K
ncf: [64, 128, 256, 256]
ninput_edges: 50000
niter: 3
niter_decay: 0
no_vis: False
norm: batch
num_aug: 1
num_groups: 1
num_threads: 2
optimizer: adam
phase: train
plus: False
pool_res: [40000, 30000, 20000, 12000]
print_freq: 9999
resblocks: 0
run_test_freq: 9999
save_epoch_freq: 1
save_latest_freq: 250
scale_verts: False
seed: 16
serial_batches: False
slide_verts: 0.0
validation: False
verbose_plot: False
verbose_train: False
which_epoch: latest
-------------- End ----------------
MeshCNN layers: Using sparse pooling
loaded mean / std from cache
#training meshes = 1
Using distributed classifier model with AdamW optimizer, GPU 0
---------- Network initialized -------------
[Network] Total number of parameters : 0.561 M
-----------------------------------------------
/mnt/homeGPU/vlugli/condaEnvs/meshcnnplus/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order, subok=True)
saving the latest model (epoch 1, total_steps 1)
saving the model at the end of epoch 1, iters 1
End of epoch 1 / 3 	 Time Taken: 40 sec
learning rate = 0.0002000
saving the latest model (epoch 2, total_steps 2)
saving the model at the end of epoch 2, iters 2
End of epoch 2 / 3 	 Time Taken: 37 sec
learning rate = 0.0000000
saving the latest model (epoch 3, total_steps 3)
saving the model at the end of epoch 3, iters 3
End of epoch 3 / 3 	 Time Taken: 37 sec
learning rate = -0.0002000
------------ Options -------------
amsgrad: False
arch: mconvnet
batch_size: 1
beta1: 0.9
checkpoints_dir: ./checkpoints
continue_train: False
csv: False
dataroot: ./data/datasets/ResTest-100K
dataset_mode: classification
dropout: [0]
epoch_count: 1
export_folder: 
fc_n: [100]
flip_edges: 0.0
gpu_ids: [0]
init_gain: 0.02
init_type: normal
is_train: True
lr: 0.0002
lr_decay_iters: 50
lr_policy: lambda
max_dataset_size: inf
name: MeshCNNPlus-ResTest-100K
ncf: [64, 128, 256, 256]
ninput_edges: 100000
niter: 3
niter_decay: 0
no_vis: False
norm: batch
num_aug: 1
num_groups: 1
num_threads: 2
optimizer: adam
phase: train
plus: False
pool_res: [80000, 60000, 40000, 24000]
print_freq: 9999
resblocks: 0
run_test_freq: 9999
save_epoch_freq: 1
save_latest_freq: 250
scale_verts: False
seed: 16
serial_batches: False
slide_verts: 0.0
validation: False
verbose_plot: False
verbose_train: False
which_epoch: latest
-------------- End ----------------
MeshCNN layers: Using sparse pooling
loaded mean / std from cache
#training meshes = 1
Using distributed classifier model with AdamW optimizer, GPU 0
---------- Network initialized -------------
[Network] Total number of parameters : 0.561 M
-----------------------------------------------
/mnt/homeGPU/vlugli/condaEnvs/meshcnnplus/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order, subok=True)
saving the latest model (epoch 1, total_steps 1)
saving the model at the end of epoch 1, iters 1
End of epoch 1 / 3 	 Time Taken: 85 sec
learning rate = 0.0002000
saving the latest model (epoch 2, total_steps 2)
saving the model at the end of epoch 2, iters 2
End of epoch 2 / 3 	 Time Taken: 78 sec
learning rate = 0.0000000
saving the latest model (epoch 3, total_steps 3)
saving the model at the end of epoch 3, iters 3
End of epoch 3 / 3 	 Time Taken: 78 sec
learning rate = -0.0002000
